---
title: "Target Markdown and stantargets for Bayesian model validation pipelines"
author: "Will Landau"
bibliography: report.bib
output:
  html_document:
    toc: true
    number_sections: true
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
```

The `targets` R package enhances the reproducibility, scale, and maintainability of data science projects in computationally intense fields such as machine learning, Bayesian Statistics, and statistical genomics [@targets]. Recent breakthroughs in the targets ecosystem make it easy to create ambitious, domain-specific, reproducible data analysis pipelines. Two highlights include Target Markdown, an R Markdown interface to transparently communicate the entire process of pipeline construction and prototyping, and `stantargets`, a new rOpenSci package that generates specialized workflows for Stan models while reducing the required volume of user-side R code [@stantargets].

# Methods

This Target Markdown report demonstrates both capabilities in an exercise to validate a mixed model of repeated measures (MMRM), one of the most ubiquitous longitudinal models in clinical trials. Using multiple simulated datasets from the prior predictive distribution of the model, we evaluate how well the model recovers the true parameters drawn from the data-generating process.
 
## MMRM model

We use a mixed model of repeated measures (MMRM) for clinical trial data. The model assumes independent patients, correlated study visits within each patient, and an LKJ prior on the correlation matrix for study visits. MMRM models with unstructured covariance usually apply the inverse Wishart prior, but this prior induces known biases that could be problematic (<https://arxiv.org/abs/1408.4050>), which motivated this custom Stan model that uses LKJ instead. 

### Notation

* $n$: number of patients.
* $t$: number of scheduled study visits for a given patient.
* $p$: number of model coefficients in the parameter vector $\beta$.
* $y$: vector of length $n \cdot t$ with observed responses for the clinical endpoint.
* $X_{(n \cdot t) \times p}$: model matrix with $n \cdot t$ rows and $p$ columns.
* $I_{x \times x}$: the identity matrix with $x$ rows and $x$ columns.
* $\otimes$: Kronecker product.
* $\Sigma_{t \times t}$: $t \times t$ block of the covariance matrix of the observed data.
* $\sigma$: vector of length $t$ with standard deviations $\sigma_1, \ldots, \sigma_t$ of the clinical endpoint at each study visit.
* $\Lambda_{t \times t}$: Cholesky factor of the $t \times t$ block of the correlation matrix of the observed data.

### Specification

$$
\begin{aligned}
& y \sim \text{MVN}(X_{(n \cdot t) \times p} \beta, \ I_{n \times n} \otimes \Sigma_{t \times t} ) \\
& \qquad \beta \sim \text{MVN} (0, 10^2 I_{p \times p})\\
&  \qquad \Sigma_{t \times t} = \left (I_{t \times t} \sigma \right ) \Lambda_{t \times t} \Lambda_{t \times t}' \left (I_{t \times t} \sigma \right ) \\
& \qquad \qquad \sigma_1, \ldots, \sigma_t \stackrel{\text{ind}}{\sim} \text{Cauchy}^+(0, 5) \\
& \qquad \qquad \Lambda_{t \times t}\Lambda_{t \times t}' \sim \text{LKJ}(\text{shape} = 1, \text{order} = t)
\end{aligned} 
$$

### Stan code

```{r}
writeLines(readLines("model.stan"))
```

## Interval-based validation

The goal of this validation exercise is to verify that the fitted model recovers the parameter values used to generate the original data. This serves as evidence that the Stan code was implemented correctly. Steps:

* For independent replication $i = 1, \ldots, r = 1000$:
  1. Draw a set of "true" parameters parameters from the joint prior.
  2. Draw a dataset from the likelihood given the parameter draws from (1).
  3. Fit the Stan model to the data.
  4. Let $x \in (0, 1)$ be a target coverage level. (In this particular workflow, we choose values 0.5 and 0.95 for $x$.) For each scalar model parameter $\theta_j$, let $c_{ij}$ = 1 if the $100 \cdot x$% posterior interval covers the prior predictive draw of $\theta_j$ from (1). Otherwise, $c_{ij}$ = 0.
* For each parameter $j$, calculate coverage: $C_j = \frac{1}{n} \sum_{i = 1}^r c_{ij}$. If $C_j$ is systematically different from $x$%, then there are problems with the model, the implementation, or the validation workflow.

This technique is similar to simulation-based calibration (SBC) [@cook2006; @talts2020]. SBC may be more robust than the interval-based validation technique above, but the latter is easier to perform with `stantargets` and easier to communicate for high-dimensional models. (Parameter-specific rank statistic histograms can be difficult to inspect when there are many parameters.)

# Construct the pipeline

This section constructs the steps of the pipeline and uses Target Markdown to explain the purpose of each step along the way. In interactive mode in the RStudio IDE, you can run each inexpensive `{targets}` code chunk in the notebook interface in order to emulate the behavior of a section of the `targets` pipeline. If you run the report in a non-interactive R session, the `{targets}` code chunks write scripts to set up the pipeline and prepare it to run later on. When it comes time to actually run the pipeline, you can do so either outside this report or by calling `tar_make()`, `tar_make_clustermq()`, or `tar_make_future()` inside an ordinary `{r}` code chunk.

## Setup

First, we load the `targets` package in the report to register the Target Markdown `knitr` language engine.

```{r, message = FALSE}
library(targets)
```

Next, we remove transient scripts previously generated by Target Markdown in non-interactive mode (optional).

```{r}
tar_unscript()
```

In our first Target Markdown code chunk, we load the packages required to define the targets in the pipeline: in this case, `targets` and `stantargets`.

```{targets packages, tar_globals = TRUE}
library(targets)
library(stantargets)
```

Next, we register the packages required to *run* the pipeline, as well as and global settings to control the storage and retrieval of data.

```{targets settings, tar_globals = TRUE}
tar_option_set(
  packages = c("tidyverse", "trialr"),
  storage = "worker",
  retrieval = "worker",
  memory = "transient",
  garbage_collection = TRUE
)
```

We also register a Sun Grid Engine (SGE) cluster with the `clustermq` package to enable distributed computing via `tar_make_clustermq()`.

```{targets clustermq, tar_globals = TRUE}
options(clustermq.scheduler = "sge", clustermq.template = "sge.tmpl")
```

## Data generation

The following function simulates one rep of a clinical trial data from the prior predictive distribution of an MMRM model. The function returns a Stan data list and accepts arguments `n_patients` (number of patients in the trial), `n_visits` (number of scheduled study visits, e.g. repeated measures per patient), and `n_arms` (number of study arms, e.g. treatment groups). The model coefficients `beta` come from independent `Normal(0, sd = 10)` distributions, and the covariance matrix comes from an `LKJ(shape = 1, order = n_visits)` distribution. To mitigate numerical stability issues, the covariance matrix is restricted such that each matrix element lies between -500 and 500 (via rejection sampling). The `.join_data` list at the end tells `stantargets` to append the prior predictive draws of model parameters to the MCMC output.

```{targets data, tar_globals = TRUE}
simulate_data <- function(n_patients = 500, n_visits = 4, n_arms = 2) {
  structure <- tibble(patient = as.character(seq_len(n_patients))) %>%
    mutate(arm = as.character(rep(seq_len(n_arms), each = n_patients / n_arms))) %>%
    mutate(covariate1 = rnorm(n()), covariate2 = rnorm(n())) %>%
    expand_grid(visit = as.character(seq_len(n_visits))) %>%
    mutate(group = paste0(arm, visit))
  x <- model.matrix(~ covariate1 + covariate2 + group, data = structure) %>%
    as.matrix()
  beta <- rnorm(ncol(x), 10)
  covariance <- NULL
  while (is.null(covariance) || max(abs(covariance)) > 100) {
    rho <- rlkjcorr(n = 1, K = n_visits, eta = 1)
    lambda <- t(chol(rho))
    sigma <- rcauchy(n = n_visits, location = 0, scale = 1)
    covariance <- diag(sigma) %*% rho %*% diag(sigma)
  }
  data <- structure %>%
    mutate(x_beta = as.numeric(x %*% beta)) %>%
    group_by(patient) %>%
    mutate(response = MASS::mvrnorm(n = 1L, mu = x_beta, Sigma = covariance)) %>%
    ungroup() %>%
    select(response, arm, patient, visit, covariate1, covariate2, group)
  list(
    data = data,
    y = data$response,
    x = x,
    n_arms = n_arms,
    n_beta = length(beta),
    n_observations = n_patients * n_visits,
    n_patients = n_patients,
    n_visits = n_visits,
    s_beta = 10,
    s_sigma = 5,
    .join_data = list(
      beta = beta,
      sigma = sigma,
      lambda = lambda
    )
  )
}
```

## Target definitions

Now, we define the targets for the steps of the workflow. We use the `stantargets` R package to manage most of the work: simulate multiple datasets, analyze each dataset with the MMRM, and calculate the coverage status of each simulation. As explained at <https://docs.ropensci.org/stantargets/articles/simulation.html>, all this can be done with the `tar_stan_mcmc_rep_summary()` function. The targets created by `tar_stan_mcmc_rep_summary()` can take a long time to run, so it is recommended to suppress interactive mode.

```{targets mcmc, tar_interactive = FALSE}
tar_stan_mcmc_rep_summary(
  name = mcmc,
  stan_files = "model.stan",
  data = simulate_data(), # Runs once per rep.
  batches = 2, # Number of branch targets.
  reps = 1, # Number of model reps per branch target.
  chains = 4, # Number of MCMC chains.
  parallel_chains = 4, # How many MCMC chains to run in parallel.
  iter_warmup = 100, # Number of MCMC warmup iterations to run.
  iter_sampling = 100, # Number of MCMC post-warmup iterations to run.
  summaries = list(
    # Compute posterior intervals at levels 50% and 95%.
    # The 50% intervals should cover prior predictive parameter draws
    # 50% of the time. The 95% intervals are similar.
    ~posterior::quantile2(.x, probs = c(0.025, 0.25, 0.75, 0.975)),
    # We use Gelman-Rubin potential scale reduction factors to
    # assess convergence:
    rhat = ~posterior::rhat(.x)
  )
)
```

Our last target calculates coverage levels and convergence diagnostics across all simulations. Because this is a single target and easy to define, we can set the `tar_simple` chunk option to `TRUE`. That way, the chunk label becomes the target name, and the chunk code becomes the target command.

```{targets coverage, tar_simple = TRUE}
mcmc %>%
  filter(!is.na(rhat), variable != "lp__") %>%
  group_by(variable) %>%
  summarize(
    coverage_50 = mean(q25 < .join_data & .join_data < q75),
    coverage_95 = mean(q2.5 < .join_data & .join_data < q97.5),
    max_rhat = round(max(rhat), 4),
    .groups = "drop"
  )
```

# Use the pipeline

Up to this point, we have only constructed the pipeline. The `{targets}` chunks write R scripts to `_targets.R` and `_targets_r/` (in non-interactive mode) but they do not actually run the pipeline. To run the pipeline, we use `tar_make()` or a related function, either outside the report or in a subsequent ordinary `{r}` code chunk.

## Inspect the pipeline

But first, it is good practice to visualize the dependency graph to verify that the workflow is correctly specified. All targets should be connected by graph edges, and they should appear in the correct order in relation to one another.

```{r graph}
tar_visnetwork()
```

## Run the pipeline

Because there are so many simulations to run, the pipeline could take a long time. Below, we use the `tar_make_clustermq()` to run different targets in parallel across multiple workers on an SGE cluster. If there are any outdated targets according to the graph above, it is recommended to run this report in a non-interactive background process (e.g. via the included shell script `run.sh`) rather than clicking the "Knit" button in the RStudio IDE. While the pipeline is running, you may wish to monitor the cluster workers with `qstat` and monitor the progress of targets using `tar_watch()` and/or `tar_poll()`.

```{r pipeline}
tar_make()
```

# Results

Finally, we report coverage statistics and convergence diagnostics. If the model is implemented correctly, `coverage_50` should not differ systematically from 0.5, `coverage_95` should not differ systematically from 0.95, and `max_rhat` should be close to 1.

```{r}
library(gt)
gt(tar_read(coverage))
```

# References