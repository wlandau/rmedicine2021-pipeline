---
title: "Target Markdown and stantargets for Bayesian model validation pipelines"
author: "Will Landau"
bibliography: index.bib
output:
  html_document:
    toc: true
    number_sections: true
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
```

The `targets` R package enhances the reproducibility, scale, and maintainability of data science projects in computationally intense fields such as machine learning, Bayesian Statistics, and statistical genomics [@targets]. Recent breakthroughs in the targets ecosystem make it easy to create ambitious, domain-specific, reproducible data analysis pipelines. Two highlights include Target Markdown, an R Markdown interface to transparently communicate the entire process of pipeline construction and prototyping, and `stantargets`, a new rOpenSci package that generates specialized workflows for Stan models while reducing the required volume of user-side R code [@stantargets].

# Methods

This Target Markdown report demonstrates both capabilities in a workflow to validate a Bayesian longitudinal linear model common to clinical trial data analysis. Using multiple simulated datasets from the prior predictive distribution of the model, we evaluate how well the model recovers the true parameters drawn from the data-generating process.
 
## The model

This particular model assumes independent patients, correlated study visits within each patient, and an LKJ prior on the correlation matrix for study visits.^[Inverse-Wishart is more common than LKJ because of conjugacy but induces a problematic prior relationship among covariance components [@alvarez2016bayesian].]

### Notation

* $n$: number of patients.
* $t$: number of scheduled study visits for a given patient.
* $p$: number of model coefficients in the parameter vector $\beta$.
* $y$: vector of length $n \cdot t$ with observed responses for the clinical endpoint.
* $X_{(n \cdot t) \times p}$: model matrix with $n \cdot t$ rows and $p$ columns.
* $\beta$: a vector of $p$ model coefficients.
* $I_{x \times x}$: the identity matrix with $x$ rows and $x$ columns.
* $\otimes$: Kronecker product.
* $\Sigma_{t \times t}$: $t \times t$ block of the covariance matrix of the observed data.
* $\sigma$: vector of length $t$ with standard deviations $\sigma_1, \ldots, \sigma_t$ of the clinical endpoint at each study visit.
* $\Lambda_{t \times t}$: Cholesky factor of the $t \times t$ block of the correlation matrix of the observed data.

### Specification

$$
\begin{aligned}
& y \sim \text{MVN}(X_{(n \cdot t) \times p} \beta, \ I_{n \times n} \otimes \Sigma_{t \times t} ) \\
& \qquad \beta \sim \text{MVN} (0, 10^2 I_{p \times p})\\
&  \qquad \Sigma_{t \times t} = \left (I_{t \times t} \sigma \right ) \Lambda_{t \times t} \Lambda_{t \times t}' \left (I_{t \times t} \sigma \right ) \\
& \qquad \qquad \sigma_1, \ldots, \sigma_t \stackrel{\text{ind}}{\sim} \text{Cauchy}^+(0, 5) \\
& \qquad \qquad \Lambda_{t \times t}\Lambda_{t \times t}' \sim \text{LKJ}(\text{shape} = 1, \text{order} = t)
\end{aligned} 
$$

### Stan code

```{r}
writeLines(readLines("model.stan"))
```

## Interval-based validation

The goal of this validation exercise is to verify that the fitted model recovers the parameter values used to generate the original data. This serves as evidence that the Stan code was implemented correctly. Steps:

* For independent replication $i = 1, \ldots, r = 1000$:
  1. Draw a set of "true" parameters parameters from the joint prior.
  2. Draw a dataset from the likelihood given the parameter draws from (1).
  3. Fit the Stan model to the data.
  4. Let $x \in (0, 1)$ be a target coverage level. (In this particular workflow, we choose values 0.5 and 0.95 for $x$.) For each scalar model parameter $\theta_j$, let $c_{ij}$ = 1 if the $100 \cdot x$% posterior interval covers the prior predictive draw of $\theta_j$ from (1). Otherwise, $c_{ij}$ = 0.
* For each parameter $j$, calculate coverage: $C_j = \frac{1}{n} \sum_{i = 1}^r c_{ij}$. If $C_j$ is systematically different from $x$%, then there are problems with the model, the implementation, or the validation workflow.

This technique is similar to simulation-based calibration (SBC) [@cook2006; @talts2020]. SBC may be more robust than the interval-based validation technique above, but the latter is easier to perform with `stantargets` and easier to communicate for high-dimensional models. (Parameter-specific rank statistic histograms can be difficult to inspect when there are many parameters.)

# Construct the pipeline

This section constructs the steps of the pipeline and uses Target Markdown to explain the purpose of each step along the way. In interactive mode in the RStudio IDE, you can run each inexpensive `{targets}` code chunk in the notebook interface in order to emulate the behavior of a section of the `targets` pipeline. If you run the report in a non-interactive R session, the `{targets}` code chunks write scripts to set up the pipeline and prepare it to run later on. When it comes time to actually run the pipeline, you can do so either outside this report or by calling `tar_make()`, `tar_make_clustermq()`, or `tar_make_future()` inside an ordinary `{r}` code chunk.

## Setup

First, we load the `targets` package in the report to register the Target Markdown `knitr` language engine.

```{r, message = FALSE}
library(targets)
```

Next, we remove transient scripts previously generated by Target Markdown in non-interactive mode (optional).

```{r}
tar_unscript()
```

In our first Target Markdown code chunk, we load the packages required to define the targets in the pipeline: in this case, `targets` and `stantargets`.

```{targets packages, tar_globals = TRUE}
library(targets)
library(stantargets)
```

Next, we register the packages required to *run* the pipeline, as well as and global settings to control the storage and retrieval of data.

```{targets settings, tar_globals = TRUE}
tar_option_set(
  packages = c("extraDistr", "tidyverse", "trialr"),
  storage = "worker",
  retrieval = "worker",
  memory = "transient",
  garbage_collection = TRUE
)
```

We also register a Sun Grid Engine (SGE) cluster with the `clustermq` package to enable distributed computing via `tar_make_clustermq()`.

```{targets clustermq, tar_globals = TRUE}
options(clustermq.scheduler = "sge", clustermq.template = "sge.tmpl")
```

## Data generation

The following function simulates one rep of a clinical trial data from the prior predictive distribution of the model. The function returns a Stan data list and accepts arguments `n_patients` (number of patients in the trial), `n_visits` (number of scheduled study visits, e.g. repeated measures per patient), and `n_arms` (number of study arms, e.g. treatment groups). To mitigate numerical stability issues, the covariance matrix is restricted (via rejection sampling) such that each response standard deviation lies is less than 10. The `.join_data` list at the end tells `stantargets` to append the prior predictive draws of model parameters to the MCMC output.

```{targets data, tar_globals = TRUE}
simulate_data <- function(n_patients = 50, n_visits = 3, n_arms = 2) {
  structure <- tibble(patient = as.character(seq_len(n_patients))) %>%
    mutate(arm = as.character(rep(seq_len(n_arms), each = n_patients / n_arms))) %>%
    mutate(covariate1 = rnorm(n()), covariate2 = rnorm(n())) %>%
    expand_grid(visit = as.character(seq_len(n_visits))) %>%
    mutate(group = paste0(arm, visit))
  x <- model.matrix(~ covariate1 + covariate2 + group, data = structure) %>%
    as.matrix()
  beta <- rnorm(n = ncol(x), mean = 0, sd = 2)
  sigma <- NULL
  while (is.null(sigma) || max(sigma) > 10) {
    rho <- rlkjcorr(n = 1, K = n_visits, eta = 1)
    lambda <- t(chol(rho))
    sigma <- rhcauchy(n = n_visits, sigma = 1)
    covariance <- diag(sigma) %*% rho %*% diag(sigma)
  }
  data <- structure %>%
    mutate(x_beta = as.numeric(x %*% beta)) %>%
    group_by(patient) %>%
    mutate(response = MASS::mvrnorm(n = 1L, mu = x_beta, Sigma = covariance)) %>%
    ungroup() %>%
    select(response, arm, patient, visit, covariate1, covariate2, group)
  list(
    data = data,
    y = data$response,
    x = x,
    n_arms = n_arms,
    n_beta = length(beta),
    n_observations = n_patients * n_visits,
    n_patients = n_patients,
    n_visits = n_visits,
    s_beta = 2,
    s_sigma = 1,
    .join_data = list(
      beta = beta,
      sigma = sigma,
      lambda = lambda
    )
  )
}
```

## Target definitions

Now, we define the targets for the steps of the workflow. We use the `stantargets` R package to manage most of the work: simulate 1000 datasets, analyze each dataset with the model, and calculate the coverage status of each simulation. We divide these 1000 replications among 100 batches for computational efficiency. As explained at <https://docs.ropensci.org/stantargets/articles/simulation.html>, all this can be done with the `tar_stan_mcmc_rep_summary()` function. The targets created by `tar_stan_mcmc_rep_summary()` can take a long time to run, so it is recommended to suppress interactive mode.

```{targets mcmc, tar_interactive = FALSE}
tar_stan_mcmc_rep_summary(
  name = mcmc,
  stan_files = "model.stan",
  data = simulate_data(), # Runs once per rep.
  batches = 100, # Number of branch targets.
  reps = 10, # Number of model reps per branch target.
  chains = 4, # Number of MCMC chains.
  parallel_chains = 4, # How many MCMC chains to run in parallel.
  iter_warmup = 2e3, # Number of MCMC warmup iterations to run.
  iter_sampling = 2e3, # Number of MCMC post-warmup iterations to run.
  summaries = list(
    # Compute posterior intervals at levels 50% and 95%.
    # The 50% intervals should cover prior predictive parameter draws
    # 50% of the time. The 95% intervals are similar.
    # We also calculate posterior medians so we can compare them
    # directly to the prior predictive draws.
    ~posterior::quantile2(.x, probs = c(0.025, 0.25, 0.5, 0.75, 0.975)),
    # We use Gelman-Rubin potential scale reduction factors to
    # assess convergence:
    rhat = ~posterior::rhat(.x)
  )
)
```

After we fit the model, we assess convergence using Gelman-Rubin potential scale reduction factors. Values far above 1 indicate convergence issues. The vast majority of Gelman factors are close to 1, so any convergence issues seem unlikely to affect coverage results.

```{targets convergence, tar_simple = TRUE}
ggplot(filter(mcmc, !is.na(rhat))) +
  geom_histogram(aes(x = rhat), bins = 30) +
  xlab("Gelman-Rubin potential scale reduction factor") +
  theme_gray(16)
```

Our last target calculates coverage levels across all simulations. Because this is a single target and easy to define, we can set the `tar_simple` chunk option to `TRUE`. That way, the chunk label becomes the target name, and the chunk code becomes the target command.

```{targets coverage, tar_simple = TRUE}
mcmc %>%
  filter(!is.na(rhat), variable != "lp__") %>%
  group_by(variable) %>%
  summarize(
    coverage_50 = mean(q25 < .join_data & .join_data < q75),
    coverage_95 = mean(q2.5 < .join_data & .join_data < q97.5),
    .groups = "drop"
  )
```

# Use the pipeline

Up to this point, we have only constructed the pipeline. The `{targets}` chunks write R scripts to `_targets.R` and `_targets_r/` (in non-interactive mode) but they do not actually run the pipeline. To run the pipeline, we use `tar_make()` or a related function, either outside the report or in a subsequent ordinary `{r}` code chunk.

## Inspect the pipeline

But first, it is good practice to visualize the dependency graph to verify that the workflow is correctly specified. All targets should be connected by graph edges, and they should appear in the correct order in relation to one another.

```{r graph}
tar_visnetwork()
```

## Run the pipeline

Because there are so many simulations to run, the pipeline could take a long time. Below, we use the `tar_make_clustermq()` to run different targets in parallel across multiple workers on an SGE cluster. If there are any outdated targets according to the graph above, it is recommended to run this report in a non-interactive background process (e.g. via the included shell script `run.sh`) rather than clicking the "Knit" button in the RStudio IDE. While the pipeline is running, you may wish to monitor the cluster workers with `qstat` and monitor the progress of targets using `tar_watch()` and/or `tar_poll()`.

```{r pipeline}
unlink("logs", recursive = TRUE)
tar_make_clustermq(workers = 100, reporter = "silent")
```

# Results

Finally, we report coverage statistics and convergence diagnostics. If the model is implemented correctly, `coverage_50` should not differ systematically from 0.5, `coverage_95` should not differ systematically from 0.95, and `max_rhat` should be close to 1.

```{r}
library(gt)
gt(tar_read(coverage))
```

# References